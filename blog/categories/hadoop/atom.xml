<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | 有间博客]]></title>
  <link href="http://caokun.me/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://caokun.me/"/>
  <updated>2013-02-07T14:05:28+08:00</updated>
  <id>http://caokun.me/</id>
  <author>
    <name><![CDATA[CaoKun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[关于Too Many fetch-failures]]></title>
    <link href="http://caokun.me/blog/2013/01/14/hadoop-too-many-fetch-failures/"/>
    <updated>2013-01-14T21:42:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/14/hadoop-too-many-fetch-failures</id>
    <content type="html"><![CDATA[<h3 id="task">查看task日志</h3>

<ul>
  <li>
    <p>task日志目录应该是 /home/mapred/cluster-data/logs/userlogs/jobid/attemptid</p>

    <pre><code>   2013-01-14 19:51:32,699 INFO org.apache.hadoop.mapred.ReduceTask: Task attempt_201211271955_3204236_r_000000_0: Failed fetch #2 from attempt_201211271955_3204236_m_009434_0
   2013-01-14 19:51:32,699 WARN org.apache.hadoop.mapred.ReduceTask: attempt_201211271955_3204236_r_000000_0 adding host hdps14035.global.alipay.com to penalty box, next contact in 8 seconds
   2013-01-14 19:51:32,699 INFO org.apache.hadoop.mapred.ReduceTask: attempt_201211271955_3204236_r_000000_0: Got 1 map-outputs from previous failures
   2013-01-14 19:51:42,703 INFO org.apache.hadoop.mapred.ReduceTask: attempt_201211271955_3204236_r_000000_0 Need another 63 map output(s) where 0 is already in progress
   2013-01-14 19:51:42,704 INFO org.apache.hadoop.mapred.ReduceTask: attempt_201211271955_3204236_r_000000_0 Scheduled 1 outputs (2 slow hosts and0 dup hosts)
   2013-01-14 19:51:42,704 INFO o
   rg.apache.hadoop.mapred.ReduceTask: Penalized(slow) Hosts: 
   2013-01-14 19:51:42,704 INFO org.apache.hadoop.mapred.ReduceTask: hdps16027.global.alipay.com Will be considered after: 18 seconds.
   2013-01-14 19:51:42,704 INFO org.apache.hadoop.mapred.ReduceTask: hdps13014.global.alipay.com Will be considered after: 0 seconds.
   2013-01-14 19:51:42,706 WARN org.apache.hadoop.mapred.ReduceTask: attempt_201211271955_3204236_r_000000_0 copy failed: attempt_201211271955_3204236_m_009434_0 from hdps14035.global.alipay.com
   2013-01-14 19:51:42,707 WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: Server returned HTTP response code: 500 for URL: http://hdps14035.global.alipay.com:50060/mapOutput?job=job_201211271955_3204236&amp;map=attempt_201211271955_3204236_m_009434_0&amp;reduce=0
      at sun.reflect.GeneratedConstructorAccessor2.newInstance(Unknown Source)
      at sun.reflect.D
   elegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
      at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
      at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1491)
      at java.security.AccessController.doPrivileged(Native Method)
      at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1485)
      at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1139)
      at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getInputStream(ReduceTask.java:1427)
      at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getMapOutput(ReduceTask.java:1357)
      at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:1270)
      at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run
   (ReduceTask.java:1200)
   Caused by: java.io.IOException: Server returned HTTP response code: 500 for URL: http://hdps14035.global.alipay.com:50060/mapOutput?job=job_201211271955_3204236&amp;map=attempt_201211271955_3204236_m_009434_0&amp;reduce=0
      at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1436)
      ... 4 more
</code></pre>
  </li>
</ul>

<h3 id="tasktrackerout">查看 tasktracker的out日志</h3>
<pre><code>     [Unloading class sun.reflect.GeneratedMethodAccessor1]
     [Unloading class sun.reflect.GeneratedConstructorAccessor3]
     [Unloading class sun.reflect.GeneratedConstructorAccessor19]
     I am failing here
     I am failing here
     [Unloading class sun.reflect.GeneratedConstructorAccessor22]
     I am failing here
     I am failing here
     [Unloading class sun.reflect.GeneratedConstructorAccessor23]
     I am failing here
     I am failing here
     I am failing here
     I am failing here
     I am failing here
     ... ... 
</code></pre>

<h3 id="tasktrackerlog">查看tasktracker的log日志</h3>
<pre><code>      2013-01-14 20:56:02,534 INFO org.apache.hadoop.mapred.TaskTracker.clienttrace: src: 10.227.6.134:50060, dest: 10.227.7.27:48058, bytes: 0, op: MAPRED_SHUFFLE, cliID: attempt_201211271955_3213399_m_000993_0
      2013-01-14 20:56:02,547 ERROR org.mortbay.log: /mapOutput
      java.util.NoSuchElementException
      	at java.util.AbstractQueue.remove(AbstractQueue.java:90)
      	at org.apache.hadoop.mapred.IndexCache.freeIndexInformation(IndexCache.java:151)
      	at org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(IndexCache.java:124)
      	at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:66)
      	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:3373)
      	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
      	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
      	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
      	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
      	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
      	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
      	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
      	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
      	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
      	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
      	at org.mortbay.jetty.Server.handle(Server.java:326)
      	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
      	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
      	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
      	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
      	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
      	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
      	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
</code></pre>

<h3 id="section">初步结论</h3>

<ul>
  <li>初步认为是tasktracker内部出异常了，需要重启所以的tasktracker</li>
</ul>

<h3 id="dumptasktracker">重启前先dump出一份tasktracker内存供后面分析</h3>
<p>　　　　　　jmap -dump:live,format=b,file=/tmp/tasktracker_heap.bin 19189</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Error List]]></title>
    <link href="http://caokun.me/blog/2013/01/09/hadoop-error-list/"/>
    <updated>2013-01-09T11:59:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/09/hadoop-error-list</id>
    <content type="html"><![CDATA[<ul>
  <li><a href="http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status/">关于Task Failed to Report Status for 601 Seconds 错误</a></li>
  <li><a href="http://caokun.me/blog/2013/01/04/hadoop-task-unassigned/">Map Task 或 Reduce Task 一直处于UNASSIGNED状态</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于Task failed to report status for 601 seconds 错误]]></title>
    <link href="http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status/"/>
    <updated>2013-01-07T11:30:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status</id>
    <content type="html"><![CDATA[<h3 id="section">问题现象</h3>

<p>Task attempt<em>201211271955</em>2680877<em>m</em>000004_0 failed to report status for 601 seconds. Killing!</p>

<p>出现这个错误的原因是：运行在Hadoop上的作业，默认配置下task需要在10分钟内汇报自己的进度，如果超过10分钟没有</p>

<p>汇报进度的话，那么taskTracker就会认为该task已经失败，并汇报给jobTracker,通常是因为处理一行记录超过了10分钟</p>

<h3 id="section-1">常规解决方法:</h3>

<p>首先可以自己打印log查看程序慢在哪一步，然后做针对性优化</p>

<p>另外也在适当的地方（比如map或reduce函数内部）加上reporter.progress()，定期汇报进度</p>

<p>如果你确认task汇报进度的时间间隔是要超过10分钟，可以修改mapred.task.timeout参数，默认值是600000(毫秒)，适当调大该值，设为0表示没有超时。</p>

<h3 id="section-2">排查过程</h3>
<ul>
  <li>
    <p>登录到hung住的节点机器</p>

    <pre><code>   $ps uxf | grep 201212271955_2688574  --查看到这个attemp task执行的子进程id 
   $jstack 16113 &gt; /tmp/js.log --查看这个子进程的jstack信息
   $vi /tmp/js.log --查看日志信息，看看这么多时间内都在干什么
</code></pre>
  </li>
</ul>

<p>可以查看到如下信息：</p>

<pre><code>     "main" prio=10 tid=0x000000004046a800 nid=0x3ef3 runnable [0x00000000417f7000]
      java.lang.Thread.State: RUNNABLE
           at org.apache.hadoop.io.Text.setCapacity(Text.java:240)
           at org.apache.hadoop.io.Text.append(Text.java:216)
           at org.apache.hadoop.util.LineReader.readLine(LineReader.java:141)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)
           - locked &lt;0x00000000e5f012c0&gt; (a org.apache.hadoop.mapred.LineRecordReader)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:273)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:101)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:108)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.doNextWithExceptionHandler(Hadoop20Shims.java:307)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.next(Hadoop20Shims.java:225)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:198)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:183)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
           at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:367)
           at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)
           at org.apache.hadoop.mapred.Child.main(Child.java:167)
</code></pre>

<p>从上面信息可以看到：</p>

<p>1：这是一个Hive SQL job</p>

<p>2: 在处理一行数据，这行数据非常长，然后text不断进行扩容来存储</p>

<ul>
  <li>Hive SQL 排查</li>
</ul>

<p>发现这个SQL是 select count(*) from tablename;这样的语句，而这个语句在另一台服务器上可以正常执行</p>

<p>通过select * from tablename limit 1; 发现显示的第一个字段内容是 SEQ”org.apache.hadoop.io.BytesWriteableorg.apache.hadoop.io.Text….这样的内容</p>

<p>这说明表内的数据实际是SequenceFile格式的，而将整个SEQ 文件作为了一列的一行数据进行了处理！</p>

<p>查看desc formatted tablename; 发现果然表模式定义中定义的是SimpleText存储格式的。问题就此定位到。</p>

<ul>
  <li>向社区提个BUG</li>
</ul>

<p><a href="https://issues.apache.org/jira/browse/HIVE-3867">HIVE-3867</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[map task 或 reduce task 一直处于UNASSIGNED状态]]></title>
    <link href="http://caokun.me/blog/2013/01/04/hadoop-task-unassigned/"/>
    <updated>2013-01-04T11:13:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/04/hadoop-task-unassigned</id>
    <content type="html"><![CDATA[<ul>
  <li>
    <p>现象是map task 或 reduce task 一直处于UNASSIGNED状态</p>
  </li>
  <li>
    <p>查看taskTracker 的jstack信息
       $JAVA_HOME/bin/jps
       9056 Child
       28402 Jps 
       13347 DataNode
       13560 TaskTracker</p>

    <pre><code>   $JAVA_HOME/bin/jstack 13560 &gt; /tmp/js_tt.log 
</code></pre>
  </li>
  <li>
    <p>jstack日志中的对应信息      <br />
       “LaunchTaskWorker for attempt<em>201211271955</em>2504854<em>m</em>000001_0” daemon prio=10 tid=0x00007fe8b8854800 nid=0x336a waiting for monitor entry [0x000000004bc66000]
          java.lang.Thread.State: BLOCKED (on object monitor)
       	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:293)
       	- waiting to lock &lt;0x00000000a00fced0&gt; (a org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext)
       	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:128)
       	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:832)
       	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2028)
       	at org.apache.hadoop.mapred.TaskTracker$LaunchTaskWorker.run(TaskTracker.java:1995)</p>

    <pre><code>   "TaskRunner for attempt_201211271955_2504482_m_000002_0" daemon prio=10 tid=0x00007fe8af69d800 nid=0x2b86 runnable [0x000000004c16b000]
      java.lang.Thread.State: RUNNABLE
   	at java.io.UnixFileSystem.createDirectory(Native Method)
   	at java.io.File.mkdir(File.java:1157)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:56)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:72)
   	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createPath(LocalDirAllocator.java:257)
   	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:335)
   	- locked &lt;0x00000000a00fced0&gt; (a org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext)
   	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:128)
   	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:251)
   	- locked &lt;0x00000000a0154ab8&gt; (a java.util.TreeMap)
   	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:180)
   	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:167)
</code></pre>
  </li>
  <li>
    <p>ps uxf，看子进程</p>
  </li>
</ul>

<p>hadoop 11031 0.0 0.0 59000 708 ? D Jan03 0:00 _du -sk /disk2/data</p>

<ul>
  <li>
    <p>问题原因</p>

    <p>disk2坏盘</p>
  </li>
  <li>
    <p>临时解决办法
       $HADOOP_HOME/bin/hadoop-deamon.sh stop tasktracker 
       如果这样终止不掉
       那么
       $kill -9 13560</p>
  </li>
  <li>
    <p>后期解决方案</p>

    <p>当tasktracker因为磁盘故障或者oom等导致task一直处于UNASSIGNED状态时，应该触发预测执行，防止job长期被hang住。</p>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop和Hive中如何确定合适的reduce task个数使得数据分发均匀]]></title>
    <link href="http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num/"/>
    <updated>2012-12-28T17:24:00+08:00</updated>
    <id>http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num</id>
    <content type="html"><![CDATA[<h3 id="hadoophash">Hadoop中计算hash值的算法</h3>
<pre><code>    package org.apache.hadoop.mapred.lib;
    import org.apache.hadoop.mapred.Partitioner;
    import org.apache.hadoop.mapred.JobConf;
    
    /** Partition keys by their {@link Object#hashCode()}. 
     */
    public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; {
    
      public void configure(JobConf job) {}
      /** Use {@link Object#hashCode()} to partition. */
      public int getPartition(K2 key, V2 value,
                              int numReduceTasks) {
        return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;
      }
    }
</code></pre>

<h3 id="hivehiveql">Hive中通过HiveQL来查看键是否分布均匀</h3>
<pre><code>	select  ( hash(card_no) &amp; 2147483647 ) % 8 as hash_code ,count(1) as cnt
	from    dwb_cbz_cs_user_grade_dd
	where   dt = '20121007'
	group by ( hash(card_no) &amp; 2147483647 ) % 8
	order by cnt  desc
	limit 10;
</code></pre>

<h3 id="reduce-task">如何确定合适的reduce task个数</h3>

]]></content>
  </entry>
  
</feed>
