<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | 有间博客]]></title>
  <link href="http://caokun.me/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://caokun.me/"/>
  <updated>2013-01-07T11:36:02+08:00</updated>
  <id>http://caokun.me/</id>
  <author>
    <name><![CDATA[CaoKun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[关于Task failed to report status for 601 seconds 错误]]></title>
    <link href="http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status/"/>
    <updated>2013-01-07T11:30:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status</id>
    <content type="html"><![CDATA[<h3 id="section">问题现象</h3>

<p>Task attempt<em>201211271955</em>2680877<em>m</em>000004_0 failed to report status for 601 seconds. Killing!</p>

<p>出现这个错误的原因是：运行在Hadoop上的作业，默认配置下task需要在10分钟内汇报自己的进度，如果超过10分钟没有</p>

<p>汇报进度的话，那么taskTracker就会认为该task已经失败，并汇报给jobTracker,通常是因为处理一行记录超过了10分钟</p>

<h3 id="section-1">常规解决方法:</h3>

<p>首先可以自己打印log查看程序慢在哪一步，然后做针对性优化</p>

<p>另外也在适当的地方（比如map或reduce函数内部）加上reporter.progress()，定期汇报进度</p>

<p>如果你确认task汇报进度的时间间隔是要超过10分钟，可以修改mapred.task.timeout参数，默认值是600000(毫秒)，适当调大该值，设为0表示没有超时。</p>

<h3 id="section-2">排查过程</h3>
<ul>
  <li>
    <p>登录到hung住的节点机器</p>

    <pre><code>   $ps uxf | grep 201212271955_2688574  --查看到这个attemp task执行的子进程id 
   $jstack 16113 &gt; /tmp/js.log --查看这个子进程的jstack信息
   $vi /tmp/js.log --查看日志信息，看看这么多时间内都在干什么
</code></pre>
  </li>
</ul>

<p>可以查看到如下信息：</p>

<pre><code>     "main" prio=10 tid=0x000000004046a800 nid=0x3ef3 runnable [0x00000000417f7000]
      java.lang.Thread.State: RUNNABLE
           at org.apache.hadoop.io.Text.setCapacity(Text.java:240)
           at org.apache.hadoop.io.Text.append(Text.java:216)
           at org.apache.hadoop.util.LineReader.readLine(LineReader.java:141)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)
           - locked &lt;0x00000000e5f012c0&gt; (a org.apache.hadoop.mapred.LineRecordReader)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:273)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:101)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:108)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.doNextWithExceptionHandler(Hadoop20Shims.java:307)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.next(Hadoop20Shims.java:225)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:198)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:183)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
           at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:367)
           at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)
           at org.apache.hadoop.mapred.Child.main(Child.java:167)
</code></pre>

<p>从上面信息可以看到：</p>

<p>1：这是一个Hive SQL job</p>

<p>2: 在处理一行数据，这行数据非常长，然后text不断进行扩容来存储</p>

<ul>
  <li>Hive SQL 排查</li>
</ul>

<p>发现这个SQL是 select count(*) from tablename;这样的语句，而这个语句在另一台服务器上可以正常执行</p>

<p>通过select * from tablename limit 1; 发现显示的第一个字段内容是 SEQ”org.apache.hadoop.io.BytesWriteableorg.apache.hadoop.io.Text….这样的内容</p>

<p>这说明表内的数据实际是SequenceFile格式的，而将整个SEQ 文件作为了一列的一行数据进行了处理！</p>

<p>查看desc formatted tablename; 发现果然表模式定义中定义的是SimpleText存储格式的。问题就此定位到。</p>

<ul>
  <li>向社区提个BUG</li>
</ul>

<p><a href="https://issues.apache.org/jira/browse/HIVE-3867">HIVE-3867</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[map task 或 reduce task 一直处于UNASSIGNED状态]]></title>
    <link href="http://caokun.me/blog/2013/01/04/hadoop-task-unassigned/"/>
    <updated>2013-01-04T11:13:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/04/hadoop-task-unassigned</id>
    <content type="html"><![CDATA[<ul>
  <li>
    <p>现象是map task 或 reduce task 一直处于UNASSIGNED状态</p>
  </li>
  <li>
    <p>查看taskTracker 的jstack信息
       $JAVA_HOME/bin/jps
       9056 Child
       28402 Jps 
       13347 DataNode
       13560 TaskTracker</p>

    <pre><code>   $JAVA_HOME/bin/jstack 13560 &gt; /tmp/js_tt.log 
</code></pre>
  </li>
  <li>
    <p>jstack日志中的对应信息      <br />
       “LaunchTaskWorker for attempt<em>201211271955</em>2504854<em>m</em>000001_0” daemon prio=10 tid=0x00007fe8b8854800 nid=0x336a waiting for monitor entry [0x000000004bc66000]
          java.lang.Thread.State: BLOCKED (on object monitor)
       	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:293)
       	- waiting to lock &lt;0x00000000a00fced0&gt; (a org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext)
       	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:128)
       	at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:832)
       	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2028)
       	at org.apache.hadoop.mapred.TaskTracker$LaunchTaskWorker.run(TaskTracker.java:1995)</p>

    <pre><code>   "TaskRunner for attempt_201211271955_2504482_m_000002_0" daemon prio=10 tid=0x00007fe8af69d800 nid=0x2b86 runnable [0x000000004c16b000]
      java.lang.Thread.State: RUNNABLE
   	at java.io.UnixFileSystem.createDirectory(Native Method)
   	at java.io.File.mkdir(File.java:1157)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:56)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:66)
   	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:72)
   	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createPath(LocalDirAllocator.java:257)
   	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:335)
   	- locked &lt;0x00000000a00fced0&gt; (a org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext)
   	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:128)
   	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:251)
   	- locked &lt;0x00000000a0154ab8&gt; (a java.util.TreeMap)
   	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:180)
   	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:167)
</code></pre>
  </li>
  <li>
    <p>ps uxf，看子进程</p>
  </li>
</ul>

<p>hadoop 11031 0.0 0.0 59000 708 ? D Jan03 0:00 _du -sk /disk2/data</p>

<ul>
  <li>
    <p>问题原因</p>

    <p>disk2坏盘</p>
  </li>
  <li>
    <p>临时解决办法
       $HADOOP_HOME/bin/hadoop-deamon.sh stop tasktracker 
       如果这样终止不掉
       那么
       $kill -9 13560</p>
  </li>
  <li>
    <p>后期解决方案</p>

    <p>当tasktracker因为磁盘故障或者oom等导致task一直处于UNASSIGNED状态时，应该触发预测执行，防止job长期被hang住。</p>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop和Hive中如何确定合适的reduce task个数使得数据分发均匀]]></title>
    <link href="http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num/"/>
    <updated>2012-12-28T17:24:00+08:00</updated>
    <id>http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num</id>
    <content type="html"><![CDATA[<h3 id="hadoophash">Hadoop中计算hash值的算法</h3>
<pre><code>    package org.apache.hadoop.mapred.lib;
    import org.apache.hadoop.mapred.Partitioner;
    import org.apache.hadoop.mapred.JobConf;
    
    /** Partition keys by their {@link Object#hashCode()}. 
     */
    public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; {
    
      public void configure(JobConf job) {}
      /** Use {@link Object#hashCode()} to partition. */
      public int getPartition(K2 key, V2 value,
                              int numReduceTasks) {
        return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;
      }
    }
</code></pre>

<h3 id="hivehiveql">Hive中通过HiveQL来查看键是否分布均匀</h3>
<pre><code>	select  ( hash(card_no) &amp; 2147483647 ) % 8 as hash_code ,count(1) as cnt
	from    dwb_cbz_cs_user_grade_dd
	where   dt = '20121007'
	group by ( hash(card_no) &amp; 2147483647 ) % 8
	order by cnt  desc
	limit 10;
</code></pre>

<h3 id="reduce-task">如何确定合适的reduce task个数</h3>

]]></content>
  </entry>
  
</feed>
