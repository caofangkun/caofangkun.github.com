<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hive | 有间博客]]></title>
  <link href="http://caokun.me/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://caokun.me/"/>
  <updated>2013-02-07T15:17:43+08:00</updated>
  <id>http://caokun.me/</id>
  <author>
    <name><![CDATA[CaoKun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[关于Task failed to report status for 601 seconds 错误]]></title>
    <link href="http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status/"/>
    <updated>2013-01-07T11:30:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/07/hadoop-hive-task-fail-to-report-status</id>
    <content type="html"><![CDATA[<h3 id="section">问题现象</h3>

<p>Task attempt<em>201211271955</em>2680877<em>m</em>000004_0 failed to report status for 601 seconds. Killing!</p>

<p>出现这个错误的原因是：运行在Hadoop上的作业，默认配置下task需要在10分钟内汇报自己的进度，如果超过10分钟没有</p>

<p>汇报进度的话，那么taskTracker就会认为该task已经失败，并汇报给jobTracker,通常是因为处理一行记录超过了10分钟</p>

<h3 id="section-1">常规解决方法:</h3>

<p>首先可以自己打印log查看程序慢在哪一步，然后做针对性优化</p>

<p>另外也在适当的地方（比如map或reduce函数内部）加上reporter.progress()，定期汇报进度</p>

<p>如果你确认task汇报进度的时间间隔是要超过10分钟，可以修改mapred.task.timeout参数，默认值是600000(毫秒)，适当调大该值，设为0表示没有超时。</p>

<h3 id="section-2">排查过程</h3>
<ul>
  <li>
    <p>登录到hung住的节点机器</p>

    <pre><code>   $ps uxf | grep 201212271955_2688574  --查看到这个attemp task执行的子进程id 
   $jstack 16113 &gt; /tmp/js.log --查看这个子进程的jstack信息
   $vi /tmp/js.log --查看日志信息，看看这么多时间内都在干什么
</code></pre>
  </li>
</ul>

<p>可以查看到如下信息：</p>

<pre><code>     "main" prio=10 tid=0x000000004046a800 nid=0x3ef3 runnable [0x00000000417f7000]
      java.lang.Thread.State: RUNNABLE
           at org.apache.hadoop.io.Text.setCapacity(Text.java:240)
           at org.apache.hadoop.io.Text.append(Text.java:216)
           at org.apache.hadoop.util.LineReader.readLine(LineReader.java:141)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)
           - locked &lt;0x00000000e5f012c0&gt; (a org.apache.hadoop.mapred.LineRecordReader)
           at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:273)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:101)
           at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)
           at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:108)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.doNextWithExceptionHandler(Hadoop20Shims.java:307)
           at org.apache.hadoop.hive.shims.Hadoop20Shims$CombineFileRecordReader.next(Hadoop20Shims.java:225)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:198)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:183)
           - locked &lt;0x00000000e5f013a0&gt; (a org.apache.hadoop.mapred.MapTask$TrackedRecordReader)
           at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
           at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:367)
           at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)
           at org.apache.hadoop.mapred.Child.main(Child.java:167)
</code></pre>

<p>从上面信息可以看到：</p>

<p>1：这是一个Hive SQL job</p>

<p>2: 在处理一行数据，这行数据非常长，然后text不断进行扩容来存储</p>

<ul>
  <li>Hive SQL 排查</li>
</ul>

<p>发现这个SQL是 select count(*) from tablename;这样的语句，而这个语句在另一台服务器上可以正常执行</p>

<p>通过select * from tablename limit 1; 发现显示的第一个字段内容是 SEQ”org.apache.hadoop.io.BytesWriteableorg.apache.hadoop.io.Text….这样的内容</p>

<p>这说明表内的数据实际是SequenceFile格式的，而将整个SEQ 文件作为了一列的一行数据进行了处理！</p>

<p>查看desc formatted tablename; 发现果然表模式定义中定义的是SimpleText存储格式的。问题就此定位到。</p>

<ul>
  <li>向社区提个BUG</li>
</ul>

<p><a href="https://issues.apache.org/jira/browse/HIVE-3867">HIVE-3867</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HIVE中map，array和structs使用]]></title>
    <link href="http://caokun.me/blog/2013/01/04/hive-array-map-structs/"/>
    <updated>2013-01-04T14:16:00+08:00</updated>
    <id>http://caokun.me/blog/2013/01/04/hive-array-map-structs</id>
    <content type="html"><![CDATA[<ul>
  <li>
    <p>关于数组的操作说明：</p>

    <pre><code>   drop table if exists table2;
   create table table2 (a array&lt;string&gt;, b array&lt;string&gt;)
   ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '\t'
   COLLECTION ITEMS TERMINATED BY ',';
     
   load data local inpath "../hive/examples/files/arraytest.txt"  overwrite into table table2;
     
   arraytest.txt中的数据形式为：（不同数组间用\t分割，同一数组内不同元素用逗号分割）
   b00,b01        b00,b01
   b00,b01        b00,b01
   b00,b01        b00,b01
   b00,b01        b00,b01
     
     
   hive&gt; select * from table2;
   OK
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   Time taken: 0.056 seconds
     
   hive&gt; select a from table2;
   OK
   ["b00","b01"]
   ["b00","b01"]
   ["b00","b01"]
   ["b00","b01"]
   Time taken: 15.903 seconds
     
   hive&gt; select a[0] from table2;
   OK
   b00
   b00
   b00
   b00
   Time taken: 12.913 seconds
     
   hive&gt; select * from table2 where a[0] = b[0];
   OK
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   ["b00","b01"]   ["b00","b01"]
   Time taken: 11.803 seconds
</code></pre>
  </li>
  <li>
    <p>关于map的操作说明：</p>

    <pre><code>   drop table if exists table2;
   CREATE TABLE table2 (foo STRING , bar MAP&lt;STRING, STRING&gt;)
   ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '\t'
   COLLECTION ITEMS TERMINATED BY ','
   MAP KEYS TERMINATED BY ':'
   STORED AS TEXTFILE;
     
   load data local inpath "../hive/examples/files/maptest.txt"  overwrite into table table2;
	 
   maptest.txt中的文件格式为：（不同列之间用一个tab分割，map中key和value用冒号分割，不同K/V间用逗号分割）
   a00        b0:b01,b1:b11
   a01        b1:b11,b2:b12
   a02        b2:b12,b3:b13
   a03        b3:b13,b4:b14
     
   select bar from table2;
   OK
   {"b0":"b01","b1":"b11"}
   {"b1":"b11","b2":"b12"}
   {"b2":"b12","b3":"b13"}
   {"b3":"b13","b4":"b14"}
   Time taken: 19.237 seconds
	 
   怎么根据 key来查询value呢？
   hive&gt; select bar['b1'] from table2;
   OK
   b11
   b11
   NULL
   NULL
   Time taken: 11.65 seconds
     
   查看map中的键值对个数：
   select size(bar) from table2;
   OK
   2
   2
   2
   2
   Time taken: 12.137 seconds
</code></pre>
  </li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop和Hive中如何确定合适的reduce task个数使得数据分发均匀]]></title>
    <link href="http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num/"/>
    <updated>2012-12-28T17:24:00+08:00</updated>
    <id>http://caokun.me/blog/2012/12/28/hive-hadoop-define-proper-reduce-num</id>
    <content type="html"><![CDATA[<h3 id="hadoophash">Hadoop中计算hash值的算法</h3>
<pre><code>    package org.apache.hadoop.mapred.lib;
    import org.apache.hadoop.mapred.Partitioner;
    import org.apache.hadoop.mapred.JobConf;
    
    /** Partition keys by their {@link Object#hashCode()}. 
     */
    public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; {
    
      public void configure(JobConf job) {}
      /** Use {@link Object#hashCode()} to partition. */
      public int getPartition(K2 key, V2 value,
                              int numReduceTasks) {
        return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;
      }
    }
</code></pre>

<h3 id="hivehiveql">Hive中通过HiveQL来查看键是否分布均匀</h3>
<pre><code>	select  ( hash(card_no) &amp; 2147483647 ) % 8 as hash_code ,count(1) as cnt
	from    dwb_cbz_cs_user_grade_dd
	where   dt = '20121007'
	group by ( hash(card_no) &amp; 2147483647 ) % 8
	order by cnt  desc
	limit 10;
</code></pre>

<h3 id="reduce-task">如何确定合适的reduce task个数</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hive安装和配置]]></title>
    <link href="http://caokun.me/blog/2012/12/28/hive-install/"/>
    <updated>2012-12-28T11:31:00+08:00</updated>
    <id>http://caokun.me/blog/2012/12/28/hive-install</id>
    <content type="html"><![CDATA[<h3 id="mysql">安装MySQL</h3>
<p>本文使用MySQL作为Hive元数据存储数据库
* <a href="http://caokun.me/blog/2012/12/28/ubuntu-install-mysql/">ubuntu下如何安装MySQL</a></p>

<h3 id="hive">获取Hive软件包</h3>
<pre><code>    ~$ wget http://mirror.bit.edu.cn/apache/hive/hive-0.9.0/hive-0.9.0.tar.gz  ./
    ~$ tar -zxvf hive-0.9.0.tar.gz
    ~$ ln -s  ./hive-0.9.0  hive-current
    ~$ mkdir hive-conf 
    ~$ cp -r ./hive-current/conf/*  ./hive-conf/
</code></pre>

<h3 id="bashprofile-bashrc">修改 ～/.bash_profile或者 ～/.bashrc,增加如下内容</h3>
<pre><code>    export HADOOP_HOME=/home/zongren/hadoop-current
    export HADOOP_CONF_DIR=/home/zongren/hadoop-conf
    export HIVE_HOME=/home/zongren/hive-current
    export HIVE_CONF_DIR=/home/zongren/hive-conf
    export ddCLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib 
    export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$ANT_HOME/bin:$FORREST_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$MVN_HOME/bin:$PATH:$HOME/bin 
</code></pre>

<h3 id="hive-1">Hive通常的配置</h3>
<p>修改$HIVE_CONF_DIR/hive-site.xml文件
          <configuration /></p>

<pre><code>      &lt;property&gt;
        &lt;name&gt;mapred.reduce.tasks&lt;/name&gt;
        &lt;value&gt;-1&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;hive.exec.reducers.max&lt;/name&gt;
        &lt;value&gt;1000&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;hive.metastore.local&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
        &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
        &lt;value&gt;root&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
        &lt;value&gt;root&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
      	&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
      	&lt;value&gt;/user/${user.name}/hive-scratchdir&lt;/value&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
        &lt;value&gt;/user/${user.name}/hive&lt;/value&gt;
        &lt;description&gt;location of default database for the warehouse&lt;/description&gt;
      &lt;/property&gt;
      
      &lt;property&gt;
        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
        &lt;description&gt;Whether to include the current database in the hive prompt.&lt;/description&gt;
      &lt;/property&gt;
      
      &lt;/configuration&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive中的touch]]></title>
    <link href="http://caokun.me/blog/2012/12/27/hive-touch/"/>
    <updated>2012-12-27T20:07:00+08:00</updated>
    <id>http://caokun.me/blog/2012/12/27/hive-touch</id>
    <content type="html"><![CDATA[<h3 id="wiki">wiki</h3>
<ul>
  <li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTableTouch">Alter Table Touch</a></li>
</ul>

<h3 id="jira">jira</h3>
<ul>
  <li><a href="https://issues.apache.org/jira/browse/HIVE-1300">HIVE-1300</a></li>
</ul>

<h3 id="section">作用</h3>
<p>读取元数据信息，然后再把元数据信息重新写入到元数据库中。</p>

<p>解决的问题场景一：
用户写了个hook来记录所有表和分区被改动的操作日志，同时有个外部脚本直接去修改了HDFS中的文件，
因为操作是在hive之外完成的，那么这个修改过程也就不会记录到操作日志中。这时这个外部脚本是可以通过调用touch命令来标记出有改动。</p>

<h3 id="section-1">使用方式</h3>
<pre><code>    ALTER TABLE tstsrc TOUCH;
    ALTER TABLE tstsrcpart TOUCH;
    ALTER TABLE tstsrcpart TOUCH PARTITION (ds='2008-04-08', hr='12');
</code></pre>

]]></content>
  </entry>
  
</feed>
